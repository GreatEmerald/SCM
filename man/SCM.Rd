% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SCM.R
\name{SCM}
\alias{SCM}
\title{Calculate a subpixel confusion-uncertainty matrix.}
\usage{
SCM(
  predicted,
  observed,
  agreement = min,
  disagreement = "SCM",
  scale = FALSE,
  accuracy = FALSE,
  totals = FALSE,
  plot = FALSE
)
}
\arguments{
\item{predicted}{A matrix or data.frame of predicted values.
Each row must sum up to 1.}

\item{observed}{A matrix or data.frame of true observed values against which
the predicted values will be validated.
Each row must sum up to 1, and must correspond to the equivalent rows in
\code{predicted}, i.e. both have to be the same length.}

\item{agreement}{A function for calculating the diagonals of the confusion matrix.
This can be any unary reduction function, such as \code{min}, taking two
numbers and returning one.
Passing \code{NULL} results in a disagreement matrix.}

\item{disagreement}{A function for calculating the off-diagonals of the confusion
matrix. If the string \code{"SCM"} is given, the disagreement is calculated
using the mean of \code{MIN_D} and \code{LEAST_D}, with the associated
uncertainty, as recommended by the Silvan-Cardenas and Wang (2008) paper.
Otherwise, the function must take four arguments: the overestimation matrix,
the underestimation matrix, and indices for the row and column of the result.
With a custom function, the uncertainty is not calculated
(a zero matrix is returned).
Passing \code{NULL} results in an agreement matrix.}

\item{scale}{Boolean, whether the result should be scaled (reported in percentages).}

\item{accuracy}{Boolean, whether to calculate accuracy statistics.
Passes the result through \code{accuracy.scm}.}

\item{totals}{Boolean, whether to calculate the row and column totals.
Implies \code{accuracy=TRUE}. Passes the result through \code{totals.scm}.}

\item{plot}{Boolean, whether to plot the resulting matrices graphically.
Passes the result through \code{plot.scm}.}
}
\value{
An object of class \code{scm} that is a list with the elements:
matrix \code{P} of the expected values of the subpixel confusion matrix,
matrix \code{U} of the uncertainty values around \code{P},
strings \code{agreement} and \code{disagreement} showing what value was
used for the \code{agreement} and \code{disagreement} arguments for
traceability, and optionally the additions from \code{accuracy.scm}
and \code{totals.scm}.
}
\description{
Calculate a subpixel confusion-uncertainty matrix.
}
\examples{
observed = c(X1=0.4, X2=0.3, X3=0.2, X4=0.1)
predictedA = c(X1=0.2, X2=0.3, X3=0.4, X4=0.1)
predictedB = c(X1=0.3, X2=0.4, X3=0.1, X4=0.2)

MIN_PROD_A = accuracy.scm(SCM(predictedA, observed, disagreement=PROD_D))
MIN_PROD_B = accuracy.scm(SCM(predictedB, observed, disagreement=PROD_D))

MIN_PROD_A[["P_overall_accuracy"]] == 0.8
MIN_PROD_B[["P_overall_accuracy"]] == 0.8

round(MIN_PROD_A[["P_kappa"]], 4) == 0.7297
round(MIN_PROD_B[["P_kappa"]], 4) == 0.7222

SCM_A = accuracy.scm(SCM(predictedA, observed))
SCM_A[["P_overall_accuracy"]] == 0.8
SCM_A[["U_overall_accuracy"]] == 0
round(SCM_A$P_kappa, 4) == 0.7297
SCM_A$U_kappa == 0

SCM_B = accuracy.scm(SCM(predictedB, observed))
round(SCM_B[["P_overall_accuracy"]], 4) == 0.8333
round(SCM_B[["U_overall_accuracy"]], 4) == 0.1667
round(SCM_B$P_kappa, 4) == 0.7778
round(SCM_B$U_kappa, 4) == 0.2222

# Example at the start of the paper; these are agreement matrices only
observed = c(Class_1=0.5, Class_2=0.375, Class_3=0.125)
predicted = c(Class_1=0.625, Class_2=0.25, Class_3=0.125)
SCM(predicted, observed, agreement="*", disagreement=NULL, totals=TRUE) # (b)
SCM(predicted, observed, agreement=LEAST, disagreement=NULL, totals=TRUE) # (c)
SCM(predicted, observed, agreement=min, disagreement=NULL, totals=TRUE) # (d)
SCM(predicted, observed, totals=TRUE, plot=TRUE) # What an SCM would look like of that example
SCM(predicted, observed, disagreement=PROD_D, totals=TRUE) # Equivalent to MIN-PROD

# From calc example
observed = c(dec.trees=32, evg.trees=32, agriculture=0,
    grass=20, water=0, urban=3, bare=10, shrubs=3) / 100
reference = c(dec.trees=30, evg.trees=40, agriculture=0,
    grass=20, water=0, urban=0, bare=10, shrubs=0) / 100
model1 = c(dec.trees=30, evg.trees=25, agriculture=2,
    grass=23, water=2, urban=3, bare=12, shrubs=3) / 100
control = c(dec.trees=12.5, evg.trees=12.5, agriculture=12.5,
    grass=12.5, water=12.5, urban=12.5, bare=12.5, shrubs=12.5) / 100

totals.scm(accuracy.scm(SCM(reference, observed, disagreement=PROD_D)))
totals.scm(accuracy.scm(SCM(reference, observed)))
scmref = SCM(reference, observed)
plot(scmref)
totals.scm(accuracy.scm(scmref))

totals.scm(accuracy.scm(SCM(model1, observed, disagreement=PROD_D, plot=TRUE)))
totals.scm(accuracy.scm(SCM(model1, observed, plot=TRUE)))

totals.scm(accuracy.scm(SCM(control, observed, disagreement=PROD_D, plot=TRUE)))
totals.scm(accuracy.scm(SCM(control, observed, plot=TRUE)))

# What if we have more than one pixel?
observed3 = rbind(observed, observed, observed)
predicted = rbind(reference, model1, control)

SCM(predicted, observed3, totals=TRUE, plot=TRUE) # 79\%
SCM(predicted, observed3, totals=TRUE, scale=TRUE) # Scaling does not affect the accuracy statistics
SCM(predicted, observed3, totals=TRUE, plot=TRUE, disagreement=PROD_D) # 79\%

# Compared to individual ones
SCM(reference, observed, totals=TRUE, plot=TRUE) # 92\%
SCM(model1, observed, totals=TRUE, plot=TRUE) # 92\%
SCM(control, observed, totals=TRUE, plot=TRUE) # 65\%
}
